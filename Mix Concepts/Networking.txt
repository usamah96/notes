Networking
--


Concept
--
OSI Model
--
Stands for Open System Interconnection Model

In the early days of networking systems, the networking devices were limited to communicate to the devices manufactured by the same vendors. For example, a 
networking device manufactured by a company X could only communicate with other devices manufactured by the same company, not with any other companiesâ€™ devices.
To ensure that the networking devices manufactured nationally and internationally can communicate with each other, the devices should be developed through a 
standard procedure, OSI Model was published by International Organization for Standardization (ISO) to bring standardization in the computing and communication system

OSI is a model that standardises the communication in the computer system.
It is what the internet runs on, mobile phone and computer uses, LAN, Router, everything that communicates with each other in computer system

Example

Lets say there is a local network and 5 devices are connected with the router with all containing different ip addresses
Machine A till Machine E

Consider Machine C running a webserver which hosts index.html page and Machine A wants to visit the page
On Machine A, we will type http://10.0.0.3:80 in the browser which will translate the request into a GET request to access Machine C server running on port 80.

What will happen in the background is the following
  .Browser sends a bunch of stuff which contains headers, cookies, content-type, etc. The Http protocol prepares all this stuff consider it a s a long string which 
   is converted into bits and send over to the network. This is the job of Application Layer (Layer 7)
  .The data is received by the Presentation Layer (Layer 6) whos job is to provide the security. If we are using https (SSL or TLS) then the necessary encryption of
   data is done here. If using http then this layer becomes optional as no encryption is done.
  .Since we are using http which is a connection based architecture which means each data has to be tagged with a session which will identify each request separately.
   So the Session Layer (Layer 5) adds a session tag (a number or anything) to the data 
  .The data then passes to the Transport Layer (Layer 4) which breaks the whole data into smaller managable segments and each segment is tagged with the source and
   destination port number. The source port is mainly auto generated by the application. Here the source port will be 80.
  .These segments are passed onto the Network Layer (layer 3) and this layer attaches 2 pieces of information to these segments. The destination and source ip
   address. These segments are now called Packets.
  .These packets are send down to Data Link Layer (layer 2) which also breaks down the packets from anywhere in the middle and on each piece it adds the destination
   and source MAC address of the machine which is now called Frames. If the MAC address of destination is not known then Address Resolution Protocol (ARP) comes into
   action which reverse engineers the ip and gets the mac address which is a 48 bit machine identifier.
  .Now the data is received by Physical Layer (Layer 1) whos job is to send these bunch of 1s and 0s (bits) over the network.
  
The tricky part is, the physical layer does not know where to send the data actual because electricity has no direction. It sends all over the network and every machine
connected to the network recevies that data. So every machine gets the data everytime all over but the beauty of OSI model is that it only allows the data 
for which it is intended to.

The receiving machine does all the steps in reverse order to get the actual data.
It get the bits onto the physical layer which passes it to Data Link Layer which checks the target MAC address onto the frame. If the target address matches then
it removes those headers and passes the data onto Network Layer and so on. If the target address does not match then the data is ignored and not forwarded as it is
not the target machine.

That is why we say don't do confidential work on publi networks because every machine is getting the data and anybody can get into it.




Concept
--
TCP vs UDP
--
TCP & UDP are communication protocl that allows us to send and receive data in a network 
They both resides in Layer 4 of OSI Model which is the Transport Protocol

TCP => Transmission Control Protocol that allow us to send information from one server to another server by specifying the ip address and
the port

Pros
  1) Acknowledgment of the message
  2) Guarenteed Delivery
  3) Connection Based
      .Client and Server need to establish a unique connection
  4) Congestion Control
      .TCP manages congesttion control as there are many traffic flowing over the netowork so when the network is too busy, the TCP will 
	   wait for network to clear a little bit to send the packed easily to avoid congestion in network.
  5) Ordered Packets
      .We know from OSI model that data is broken down into smaller chunks of packets which are sent over the network. So every packet
	   received to the server might be un-ordered as every packet can take a different route to reach to the server. TCP will ensure that
	   packets after reaching to the server will get in ordered once received all of them.
	   
	   
Cons
  1) Larger Packets
      .TCP does overhead by adding all these data onto packets for secure, guarenteed delivery and acknowledgments.
  2) More Bandwidth
      .Larger packet means more bandwidth required. SO tough luck when using 2G or 3G network.
  3) Slower Than UDP
      .Slower because we are always waiting. Waiting for acks, waiting for congestion control, waiting for guarenteed delivery, etc
  4) Stateful
      .Both server and client carries the information about the connection. If I shut down the server, the connection is closed. The client
	   request is failed saying connection not reachable as server is down. You can reconnect but you lost the previous connection
  5) Server Memory
      .Due to connection based and statefulness, server assigns a memory to every establishing connection and that is why there is a 
       connection limit in TCP based connection.


We can use telnet to try and test our TCP server.
Telnet uses TCP to connect to a remote or a local system.


UDP => User Datagram Protocol

Cons
  1) No Acknowledgment
  2) No Guarenteed Delivery
      .Adds a checksum which only tells the server whether the packet is valid/good or invalid/bad. It gives an option to the server
	   whether to accept the packet or to drop it.
  3) Connectionless
      .The server and client don't know each other
  4) No Congestion Control
      .Does not wait and does not care if the network is busy or not. It just keeps on sending the packet making network congested.
  5) No Ordered Packets
      .Does not wait for all the packets to arrive and order them. Does not adds extra headers or sequence to identify
  6) Security
      .As there is no connection, anyone can jump in and send stuff to the server.
	  
	  
Pros
  1) Smaller Packets
      .No connection, no congestion, no ordering technique and hence packets are smaller.
  2) Less Bandwidth
      .Smaller packets means low bandwidth. Weak network supports it. Some games uses UDP to support weak network also.
  3) Faster Than TCP
      .No waiting for anything.
  4) Stateless
      .As no information is stored on either client or server. If server dies and comes back up, the client can continue sending
	   the information without knowing that the server went down.
	   



Concept
--
About TCP Handshake
--
Why need TCP Handshake?

Whenever we issue a Curl request or go to a web browser and issue an Http request the first thing that happens is we establish a TCP
connection (relevant before Http 3 because Http 3 moves towards UDP)

Lets say we issue a GET request which have headers, cookies, parameters etc, and eventually it turns into a bunch of packets. The 
packets could be 3, 6 7 and many more that is sent one by one. The server will receive those packets and process the request. But before
that lets assume that one of the packet did not arrive at the server. How will the client know that certain packet did not reached the
server? Here we will have to introduce a concept of Achnowledgment that TCP Protocol provides. Each packat sent will be followed by an
Acknowledgment

If one of the packet did not receives the acknowledgment to the client then the client retries the request and that is the feature of the
TCP otherwise we will run into corrpution all the time. Another question arises here is that how each packet is identified? Here the
concept of sequence number comes. Server will wait for all the packet to arrive and order them all by sequence number to be uniform. But
how does the server know that packet with sequence 5 is after packet with sequence 4 in order to order them? Here an important concept 
of Synchronization between client and server comes in

The client and the server will have to be synchroinzed and agree on the same sequencing as client and server can contain different
sequencing. For this reason we need TCP handshake
The sequencing is created on random basis like starting from 856, 857, 858 to prevent from reply attacks and syn attacks. We dont want
to give attacker hints by following a pattern for sequencing

As we now know that client and server needs to agree on sequencing, here is how TCP 3 way handshake takes place.
  .The Client sends a TCP request to Server telling his sequencing, lets say a request SYN 700.
  .The Server will now need to do 2 things. Acknowledge the SYN 700 and sends back its sequencing. So the client sends a TCP request
   with (ACK 700 + length of SYN 700 request also known as the ghost byte) and SYN 200
  .Now the Client needs to acknowledge request coming from server so it sends back TCP request with (ACK 200 + length of SYN 200)
  
Now the client will send the actual GET request with starting sequence number as 700 which will turn into packets and all packets
will get subsequent sequence number.
The server will send back ACK of that GET request which is not the actual response of the request but it is only the way of telling to
the client that you can safely disconnect as I have received your reqeust and now I will process it.

Video Ref => https://www.youtube.com/watch?v=bW_BILl7n0Y&list=PLQnljOFTspQX_Zkt_8teMRsdY4sNt4BX6&index=4




Concept
--
TCP Slow Start
--
TCP introduces a feature called 'Slow Start' which is used to control the congestion into the network. Basically to handle the amount of packets
sent from client (could be curl, browser, any application etc) to server
	   
How slow start actually works?
So TCP, when sending the data, starts off with a very small windows size packet and sents over to the network. If the packet is acknowledged by the
server, TCP starts to very gradually increase the window size of the packet to be sent over the network. Now lets say at a certain window size, if
the packet does not gets the acknowledgment, TCP knows now here to stop and not to increase window size much so it rather keeps on decreasing the
the size to control the congestion in the network.

Slow Start is a good feature to handle congestion control but it can ultimately affect our performance.
  .If we are dealing with small request and small amount of body in a request, slow start will not hinder the performance
  .But if we are dealing with large payloads like uploading a 1GB file then we will see some some performance hindrence because initially TCP will
   send very tiny small packets of data to handle congestion control and slowly increase the packet size which will obviously increase the time for all
   packets to reach the server.

There is a way to overcome this which is to warm up the TCP connection. Before sending actual request, we set up a proxy in which we open TCP
connections which tries and tests the congestion control and when then the actual requests comes in we directly sent a maximum packet window to boost
the performance.



Concept
--
TCP Fast Open
--
TCP Fast Open is a feature which can spped up our application performance.
Here is how it works,

We know that for every request, we need a 3 way TCP Handshake with the server. the 3 way handshake can be considered as slow becaue it is done
every time if we talk before Http 2.0. What if the SYN, ACK handshake packets are lost or blocked by firewall or dropped or anything can happen.

Modern servers and client can support TCP Fast Open feature which actually allow us to send the actual request along with handshaking simultaneosuly.
What will happen is that the client will request to open tcp connection with the server by sending the SYN using the fast-open command and the 
server will recoginze it that the client needs fast-open connection. If server does not supports fast-open then it will ignore it, otherwise the 
server will send back the Fast Open Cookie in the ACK request to the client for which client will recognize that the server supports fast-open.
Now the client has Fast Open Cookie, whenever the client needs to send the GET/POST request it will attach Fast Open Cookie with the SYN request 
along with the GET/POST request for which the server will ACK it and then the actual request will be processed. So no waiting for 3 way handshake,
all data given to server within client SYN request to the server.

Example command.
curl -tcp-fast-open http://google.com

This is applicable when using Http < 2.0 because with Http 3.0 it used QUIC (UDP) and Http 2.0 uses Multiplexing so one TCP connection is enough for
all the requests.


Video Ref => https://www.youtube.com/watch?v=G2erltVFchE&list=PLQnljOFTspQX_Zkt_8teMRsdY4sNt4BX6&index=7




Concept
--
TCP Half Open
--
In TCP Half Open, we leave the TCP handshake in the middle. Typlically used by Port Scanning applications

Example
Lets say there is an application which checks whether a certain port is opened by server or not.
  .We have a server which has ports 80, 443 and 22 open.
  .The client opens a TCP connection and sends a SYN request to port 80
  .The server sends back SYN/ACK which means the port is opened and then the client closes the connection. It does not send back ACK to the server to
   complete the 3 way handshake.
  .The server will retry sending the SYN/ACK and then closes the connection after sometime if it does not receive ACK from the client.
  .This is where the server is Half Opened.
  
This Half Open connection is dangerous because it can lead to SYN Flood Attack.
If the client is same, we know that the SYN TCP request containts source IP and port and destination IP and port. So the server will recognize that
client is not responding everytime for SYN request, it will block that client because Half Open connection takes up memory at the server.

Now what if there is a malicious client who spoofs the client ip and sends SYN request to the server in a distributed environment much like a DDOS.
Client (ip = z and port 123) sends these request to the server (ip = a and port 80).
  1) SYN (source ip = b and port = 12, destination ip = a and port 80)
       .Server responds with SYN/ACK (source ip a and port 80, destination ip b and port 12)
  2) SYN (source ip = c and port = 12, destination ip = a and port 80)
       .Server responds with SYN/ACK (source ip a and port 80, destination ip c and port 12)
  3) SYN (source ip = d and port = 12, destination ip = a and port 80)
       .Server responds with SYN/ACK (source ip a and port 80, destination ip d and port 12)
  4) SYN (source ip = e and port = 12, destination ip = a and port 80)
       .Server responds with SYN/ACK (source ip a and port 80, destination ip e and port 12)
 ----------------
 ----------------
 ----------------
 
All these hald opened connection takes up space and will make the server flooded and a legit user will not be able to make 3 way handshake with the
server.
Firewalls mainly detect this and prevent it with deep packet analysis. They notice that hey, the server been sending SYN/ACKs but those clients are 
not replying so let us block this kind of traffic. Machine learning plays big rule here.
There is also a low level TCP timeout (SYN/ACK) timeout but the problem you don't want this to be set to very low otherwise normal connections 
will suffer. 
The DDOS providers already has the functionality built in to prevent SYN flood attacks.




Concept
--
Http Versions (0.9 v 1.0 v 1.1 v 2.0 v 3.0)
--
HTTP is an application protocol (layer 7 OSI and layer 4 TCP/IP) employed to create distributed hypermedia systems. 
It emerged as a simple protocol to enable communication between clients and servers over the Internet.
It relies on TCP/IP to work.

The first release of HTTP (0.9) was much limited. This version only enabled clients to request information from a server using a single 
operation: GET.
The first HTTP release only supported transmitting ASCII data.


Version 1.0 of HTTP brings several new utilities.

  1) Header
      Only the method and the resource name composed an HTTP 0.9 request. HTTP 1.0, in turn, introduced the HTTP header, thus allowing 
	  the transmission of metadata that made the protocol flexible and extensible
  2) Versioning
      The HTTP requests explicitly informs the employed version, appending it in the request line
  3) Status Code
      HTTP responses now contain a status code, thus enabling the receiver to check the request processing status (successful or failed)
  4) Content-type
      In specific to the Content-Type field, HTTP can transmit other documents types than a plain HTML file
  5) New Methods
      Besides GET, HTTP 1.0 provides two new methods (POST and HEAD)
	  
	  
With Version 1.1, the most relevant enhancements were,
  1) Persistent Connections
      In HTTP 1.0, each request/response pair requires opening a new connection. In HTTP 1.1, it is possible to execute several requests 
	  using a single connection
  2) New Methods
      Besides the already available methods of HTTP 1.0, the 1.1 version added six extra methods: PUT, PATCH, DELETE, CONNECT, TRACE, and OPTIONS
  

HTTP 2.0 focused on improving the protocol performance. It implemented several features to improve connections and data exchange.
  1) Request Multiplexing
      HTTP 1.1 is a sequential protocol. So, we can send a single request at a time. HTTP 2.0, in turn, allows to send requests and receive responses 
	  asynchronously. In this way, we can do multiple requests at the same time using a single connection,
  2) Request Prioritization
      With HTTP 2.0, we can set a numeric prioritization in a batch of requests. Thus, we can be explicit in which order we expect the responses, 
	  such as getting a webpage CSS before its JS files
  3) Automatic Compression
      HTTP 2.0, executes a GZip compression automatically. Http 1.1 requires the compression of requests and responses explicitly
  4) Connection Reset
      A functionality that allows closing a connection between a server and a client for some reason, thus immediately opening a new one
  5) Server Push
      To avoid a server receiving lots of requests, HTTP 2.0 introduced a server push functionality. With that, the server tries to predict the
	  resources that will be requested soon. So, the server proactively pushes these resources to the client cache.
	  Lets say with index.html we need main.js and main.css so there is no need to issue a separate request for js and css file. With index.html
	  request the server can send back main.js and main.css files too. But that can cause problems if our browser does not support Http 2.0 and
	  dont know how to receive the request.
	  
	  
As we know that in Http 1.1, we cannot issue another request per TCP connection once the previous request comes with a response. So modern browsers
do a hack is that whenever there are multiple requests, they open 6 new TCP connections to issue 6 requests in parallel with the server.
With Http 2.0 using the multiplexing feature we can issue parallel requests per TCP connection 

Http 1.1 vs Http 2.0 Performance Demo => https://www.youtube.com/watch?v=fVKPrDrEwTI&list=PLQnljOFTspQWbBegaU790WhH7gNKcMAl-
	  
	  
Http 3.0 is different from the previous HTTP versions
The main difference between HTTP 2.0 and HTTP 3.0 is the employed transport layer protocol. In HTTP 2.0, we have TCP connections with or not 
TLS (HTTPS and HTTP). HTTP 3.0, in turn, is designed over QUIC (Quick UDP Internet Connections).
HTTP 3.0 replaces the transport layer protocol employed by HTTP from its first version, TCP, with a new one: QUIC.

Relevant characteristic of HTTP 3.0 is that it always creates encrypted connections. So, it is similar to always employing HTTPS in current HTTP 2.0.  




Concept
--
How Https Works?
--
Several steps are there before sending actual https request to the server

1) Computers agree on how to encrypt the data
     .Client send server a Hello message which includes details about how to encrypt the data using Key (RSA, Diffie Hellman, DSA), Cipher (RC4, AES),
	  Hashing Technique (HMAC-MD5, HMAC-SHA), TLS version (1.2, 1.3)
	 .The Server chose one of the technique for encryption

2) Server sends client its signed certificate
    .After agreeing on encryption technique, TLS version server sends back certificate details (issuer, certificate authority, validity, 
	 public key, etc)
	.Client confirms that it accepts the certificate. It generates a new secret key and encrypts it with server's public key and send it to server
	.Server decrypts it with its private key and gets the secret key generated by the client
	.Now server and client are the only 2 machines having the secret key all over the internet from which communication takes place.
	
3) Encryption is started and data transmission can began
    .All of the messages gets encrypted using secret key generated by the client.

Most browsers already have built in trusted certificate authorities like Google CA (used by Youtube)

When on stagind environment, we test https functionality using self-signed certificate.
   .We set up our own Certificate Authority by generating server's public and private key
   .Client browser warns for not trusted party before entering the server
   
On production, we use a trusted certificate authority.
  .We send a signing certificate request to google with the generated key pair.
  .The Google CA accepts the request and signs thae certificate with its private key (to ensure that it is by google and not any other)
  .Client varifies it with Google CA's public key that signature is valid and preservs the certificate.


TLS / SSL is cryptographic protocol used to send data over a network securely
TLS is just a name given to updated version of SSL.
  **
   Using TLS 1.2 takes 2 round trips for TCP Handshake. TLS 1.3 optimizes the hadshake to 1 round trip
   Everything before TLS 1.2 is being deprecated due to security flaws
   Current Version of TLS is 1.3 released in 2018
  **

https://www.youtube.com/watch?v=T4Df5_cojAs
https://www.youtube.com/watch?v=67kItGjvRs0&t=311s




Concept
--
SSH
--
Protocol to run the commands on the remote server in a secure way
We establish a connection to the remote server from our local computer using ssh